{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64bc5255",
   "metadata": {},
   "source": [
    "## TARS iterative model development\n",
    "- bootstrap a more complex model with a series of cheap models and search/quota oriented sampling/labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfcace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T04:59:58.465586Z",
     "start_time": "2021-09-09T04:59:53.413963Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from otso_dictionary_wrapper.model import DictionaryClassifier\n",
    "from otso_flair_wrapper.model_dev import *\n",
    "from otso_nlp_util.experiment_creation import (\n",
    "    create_experiment_output_dir,\n",
    "    create_multi_class_train_test_splits,\n",
    "    create_multi_label_train_dev_test_splits,\n",
    "    create_multi_label_train_test_splits,\n",
    "    get_multi_label_train_dev_test_frames,\n",
    ")\n",
    "from otso_nlp_util.experiment_evaluation import (\n",
    "    compare_experiments_barplot,\n",
    "    create_multi_label_classification_report,\n",
    ")\n",
    "from otso_nlp_util.label_processing import *\n",
    "from otso_nlp_util.label_processing import (\n",
    "    label_dictionary_to_label_mat,\n",
    "    label_lists_to_label_dictionary,\n",
    "    one_hot_encode_multi_label_df,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e67e3",
   "metadata": {},
   "source": [
    "## Annotation bootstrapping\n",
    "- via a cheap dictionary model; though any model can be used that implements the `predict_single` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759167e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T04:26:32.546743Z",
     "start_time": "2021-09-09T04:26:32.543373Z"
    }
   },
   "outputs": [],
   "source": [
    "from clear_bow.classifier import DictionaryClassifier\n",
    "\n",
    "dictionaries = {\n",
    "    \"customer_service\": [\"customer service\", \"service\", \"experience\"],\n",
    "    \"pricing\": [\"expensive\", \"cheap\", \"dear\", \"dollars\", \"cents\"],\n",
    "    \"billing\": [\"quarterly\", \"online\", \"phone\"],\n",
    "    \"product\": [\n",
    "        \"quality\",\n",
    "        \"product\",\n",
    "        \"superior\",\n",
    "        \"inferior\",\n",
    "        \"fast\",\n",
    "        \"efficient\",\n",
    "        \"range\",\n",
    "        \"selection\",\n",
    "        \"replaced\",\n",
    "    ],\n",
    "    \"competitor\": [\"another provider\", \"competitor\", \"leaving\", \"will not return\"],\n",
    "}\n",
    "dc = DictionaryClassifier(classifier_type=\"multi_label\", label_dictionary=dictionaries)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18857d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7554bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def consolidate_hard_soft_labels(label_objects):\n",
    "    # defer to hard labels where they exist, otherwise average soft labels\n",
    "    label_objects = [e for e in label_objects if type(e) == dict]\n",
    "    set.union(*[set(e) for e in label_objects])\n",
    "\n",
    "    flattened_object = {}\n",
    "    all_labels = pd.DataFrame(label_objects)\n",
    "    for e in all_labels:\n",
    "        # hard reject\n",
    "        if -1 in set(all_labels[e]):\n",
    "            flattened_object[e] = -1\n",
    "        # hard accept\n",
    "        elif 1 in set(all_labels[e]):\n",
    "            flattened_object[e] = 1\n",
    "        # mean otherwise\n",
    "        else:\n",
    "            flattened_object[e] = all_labels[e].mean()\n",
    "\n",
    "    return flattened_object\n",
    "\n",
    "\n",
    "def verify_n_label_object_examples(\n",
    "    df,\n",
    "    label_col,\n",
    "    label_object_value,\n",
    "    accept_value: str = \"y\",\n",
    "    reject_value: str = \"n\",\n",
    "    n_examples: int = 10,\n",
    "):\n",
    "    # given a list of records, positively verify (binary confirmation) across a selected field until examples run out/quota reached. Return all annotations.\n",
    "    updated_records = []\n",
    "    print(\n",
    "        f\"Input an: '{accept_value}' to accept value, input {reject_value} to reject value. \"\n",
    "        + f\"Label objects saved as '{label_col}' field within all records. \"\n",
    "        + f\"Loop will break when '{n_examples}' positively affirmed or examples run out, whichever first. {df.shape[0]} candidates supplied.\\n\"\n",
    "    )\n",
    "    for idx, e in df.iterrows():\n",
    "        # break if positive n_examples found\n",
    "        if (\n",
    "            len([e for e in updated_records if e[label_col][label_object_value] == 1])\n",
    "            >= n_examples\n",
    "        ):\n",
    "            return pd.DataFrame(updated_records)\n",
    "\n",
    "        # always use a deep copy\n",
    "        d = e.to_dict()\n",
    "        dc_d = copy.deepcopy(d)\n",
    "        _ = [print(\"\\033[1m\", k, \": \", \"\\033[0m\", v) for k, v in d.items()]\n",
    "        val = input(f\"Instance of {label_object_value}? \")\n",
    "        verification_remapping = {\"y\": 1, \"n\": -1, \"\": 0.0, \" \": 0.0}\n",
    "        dc_d[label_col][label_object_value] = verification_remapping.get(val)\n",
    "        updated_records.append(dc_d)\n",
    "\n",
    "        # clear_output()\n",
    "    return pd.DataFrame(updated_records)\n",
    "\n",
    "\n",
    "def consolidate_doc_labels(df, label_col):\n",
    "    # convenience function\n",
    "    label_objects = df[label_col].tolist()\n",
    "\n",
    "    if len([e for e in label_objects if type(e) == dict]) == 0:\n",
    "        # NAN values - usually from concatenated, unseen records\n",
    "        df[label_col] = [None] * df.shape[0]\n",
    "    else:\n",
    "        df[label_col] = [consolidate_hard_soft_labels(label_objects)] * df.shape[0]\n",
    "    return df.head(1)\n",
    "\n",
    "\n",
    "def annotate_n_examples_per_class(\n",
    "    model,\n",
    "    df,\n",
    "    text_col,\n",
    "    n_examples=10,\n",
    "    specific_labels=[],\n",
    "    prediction_thresh=0.75,\n",
    "    rank_candidates=True,\n",
    "    max_candidates=50,\n",
    "    label_col=\"mixed_labels\",\n",
    "):\n",
    "    # 1. label book-keeping - restrict against a pre-existing set of labels\n",
    "    model_labels = [\n",
    "        e for e in model.predict_single(\"hello world\").keys() if e != \"no_label\"\n",
    "    ]\n",
    "    if specific_labels:\n",
    "        # sanity check the specified labels against available model labels\n",
    "        label_intersection = sorted(\n",
    "            list(set(specific_labels).intersection(set(model_labels)))\n",
    "        )\n",
    "        if not label_intersection:\n",
    "            raise ValueError(\"No label intersection found, aborting sampling\")\n",
    "        print(f\"Specified: {sorted(specific_labels)}\")\n",
    "        print(f\"Model features: {sorted(model_labels)}\")\n",
    "        print(f\"Sampling using the intersecting labels only: {label_intersection}\")\n",
    "        pred_labels = label_intersection\n",
    "    else:\n",
    "        pred_labels = model_labels\n",
    "\n",
    "    # 2. drop duplicates on text field or consolidate labels, consolidate multi-document records\n",
    "    if label_col in df:\n",
    "        df = (\n",
    "            df.groupby(text_col)\n",
    "            .apply(lambda x: consolidate_doc_labels(x, label_col))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        df = df.copy(deep=True).drop_duplicates(text_col)\n",
    "\n",
    "    if label_col in df:\n",
    "        # 3.1 consolidate old verifications with new predictions, defer to old verifications\n",
    "        consolidated_labels = []\n",
    "        for hard, soft in zip(\n",
    "            df[label_col].tolist(), df[text_col].apply(model.predict_single).tolist()\n",
    "        ):\n",
    "            if type(hard) != dict:\n",
    "                # RE: iteratively appending datasets\n",
    "                consolidated_labels.append(soft)\n",
    "            else:\n",
    "                consolidated_labels.append(consolidate_hard_soft_labels([hard, soft]))\n",
    "        df[label_col] = consolidated_labels\n",
    "    else:\n",
    "        # 3.2 otherwise create new prediction\n",
    "        df[label_col] = df[text_col].apply(model.predict_single)\n",
    "\n",
    "    all_verifications = []\n",
    "    for e in pred_labels:\n",
    "        # 4. only use unseen examples, if verification field exists\n",
    "        seen_examples = None\n",
    "        if label_col in df:\n",
    "            seen_examples = df[\n",
    "                df[label_col].apply(lambda y: True if type(y[e]) == int else False)\n",
    "            ]\n",
    "            unseen_examples = df[\n",
    "                ~df[label_col].apply(lambda y: True if type(y[e]) == int else False)\n",
    "            ]\n",
    "            if seen_examples.shape[0] > 1:\n",
    "                print(\n",
    "                    f\"{seen_examples.shape[0]} pre-existing, positive examples found for {e}\"\n",
    "                )\n",
    "\n",
    "        # adjust n-examples to retrieve\n",
    "        adjusted_n_examples = (\n",
    "            n_examples - seen_examples.shape[0]\n",
    "            if seen_examples.shape[0] > 0\n",
    "            else n_examples\n",
    "        )\n",
    "\n",
    "        # 5. only annotate examples with relatively high confidence\n",
    "        annotate_input_temp = (\n",
    "            unseen_examples\n",
    "            # only examine examples above prediction threshold\n",
    "            .pipe(\n",
    "                lambda x: x[\n",
    "                    x[label_col].apply(\n",
    "                        lambda y: True if y[e] >= prediction_thresh else False\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if rank_candidates:\n",
    "            # 6.1 rank by prediction confidence\n",
    "            annotate_input_temp = annotate_input_temp.reset_index(drop=True).pipe(\n",
    "                lambda x: x.iloc[\n",
    "                    x[label_col]\n",
    "                    .apply(lambda x: x[e])\n",
    "                    .sort_values(ascending=False)\n",
    "                    .index\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # 6.2 shuffle candidates otherwise\n",
    "            annotate_input_temp = annotate_input_temp.sample(frac=1.0, random_state=42)\n",
    "\n",
    "        # 7. take the first n=max_candidate records\n",
    "        annotate_input_temp = annotate_input_temp.head(max_candidates)\n",
    "\n",
    "        # 8. actually annotate the filtered data..\n",
    "        if annotate_input_temp.shape[0] == 0:\n",
    "            print(f\"\\n\\n****\\t No candidate examples found for: {e}, skipping\\t****\\n\")\n",
    "            annotations = pd.DataFrame()\n",
    "        else:\n",
    "            print(f\"\\n\\n****\\t Annotating: {e} \\t****\\n\")\n",
    "            annotations = verify_n_label_object_examples(\n",
    "                annotate_input_temp, label_col, e, n_examples=adjusted_n_examples\n",
    "            )\n",
    "\n",
    "        # 9. consolidate alongside seen examples; grow pool of annotations\n",
    "        if seen_examples.shape[0] > 1:\n",
    "            annotations = pd.concat([seen_examples, annotations])\n",
    "\n",
    "        all_verifications.append(annotations)\n",
    "\n",
    "    # 10. consolidate across records\n",
    "    return (\n",
    "        pd.concat(all_verifications)\n",
    "        .groupby(text_col)\n",
    "        .apply(lambda x: consolidate_doc_labels(x, label_col))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def backfill_multi_label_objects(\n",
    "    model, df, text_col, label_col, specific_labels=None, prediction_thresh=0.75\n",
    "):\n",
    "    # Given an original set of labels (labels a), verify and backfill complimentary model predictions (labels b). Useful to \"pad\" multi-label datasets with consistent labelling.\n",
    "    if specific_labels is None:\n",
    "        specific_labels = []\n",
    "    df = df.copy(deep=True)\n",
    "\n",
    "    # assume normalized label space\n",
    "    label_space_b = [\n",
    "        e for e in model.predict_single(\"hello world\").keys() if e != \"no_label\"\n",
    "    ]\n",
    "    if specific_labels:\n",
    "        # sanity check the specified labels against available model labels\n",
    "        target_label_space = set(specific_labels).intersection(set(label_space_b))\n",
    "    else:\n",
    "        target_label_space = set(label_space_b)\n",
    "\n",
    "    assert target_label_space  # at least one label\n",
    "    target_label_space = sorted(list(target_label_space))\n",
    "\n",
    "    consolidated_labels = []\n",
    "    for hard, soft in zip(\n",
    "        df[label_col].tolist(), df[text_col].apply(model.predict_single).tolist()\n",
    "    ):\n",
    "        if type(hard) != dict:\n",
    "            # RE: iteratively appending datasets\n",
    "            consolidated_labels.append(soft)\n",
    "        else:\n",
    "            consolidated_labels.append(consolidate_hard_soft_labels([hard, soft]))\n",
    "\n",
    "    for label, (idx, label_object) in itertools.product(\n",
    "        target_label_space, enumerate(consolidated_labels)\n",
    "    ):\n",
    "        if label_object[label] in [-1, 1]:\n",
    "            # 1. pre-existing hard label, no change\n",
    "            continue\n",
    "\n",
    "        elif label_object[label] >= prediction_thresh:\n",
    "            # 3. otherwise, some difference in labels, as proposed by model\n",
    "            clear_output()\n",
    "            print(f\"**** Verifying all additional instances of: {label} ****\")\n",
    "            print(f\"\\n\\033[1mText: \\033[0m \\n{df.iloc[idx][text_col]}\\n\")\n",
    "            # bold does not work in input\n",
    "            print(f\"\\033[1mInstance of {label}?\\033[0m\")\n",
    "            confirmation = input()\n",
    "\n",
    "            if confirmation == \"n\":\n",
    "                label_object[label] = -1\n",
    "\n",
    "            elif confirmation == \"y\":\n",
    "                # assign in place.. yikes\n",
    "                label_object[label] = 1\n",
    "    # reassign\n",
    "    df[label_col] = consolidated_labels\n",
    "    return df\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e653984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T04:26:58.256565Z",
     "start_time": "2021-09-09T04:26:35.763641Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify model predictions for n examples, optionally tweak how candidate examples are retrieved\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "\n",
    "stateful_annotations = annotate_n_examples_per_class(\n",
    "    dc,\n",
    "    df,\n",
    "    text_col,\n",
    "    n_examples=2,\n",
    "    prediction_thresh=0.7,\n",
    "    rank_candidates=True,\n",
    "    max_candidates=2,\n",
    "    label_col=label_col,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # re-iterate annotations on the same dataset (pre-existing experiment?), updating as needed\n",
    "# concat = pd.concat([(df\n",
    "#                     .query('new_text not in @stateful_annotations.new_text')\n",
    "#                     ),\n",
    "#                    stateful_annotations])\n",
    "# stateful_annotations = annotate_n_examples_per_class(dc, concat, text_col, n_examples=10, prediction_thresh=0.75, rank_candidates=True, max_candidates=20)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if multi-label, backfill across the label space\n",
    "stateful_annotations = backfill_multi_label_objects(\n",
    "    dc, stateful_annotations, text_col, label_col, prediction_thresh=0.7\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7aa346",
   "metadata": {},
   "source": [
    "## Flair zero/few-shot learning\n",
    "- Format tiny datset as an experimernt, train, eval etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cff7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize annotations into usual format; some data loss here (explicit rejections => \"-1\" values)\n",
    "quantized_annotations = stateful_annotations.copy(deep=True)\n",
    "quantized_annotations[label_col] = quantized_annotations[label_col].apply(\n",
    "    threshold_one_hot_dictionary\n",
    ")\n",
    "quantized_annotations[label_col] = normalize_label_space(\n",
    "    quantized_annotations[label_col].tolist()\n",
    ")\n",
    "\n",
    "quantized_annotations = (\n",
    "    quantized_annotations\n",
    "    # ensure each record has at least one record\n",
    "    .reset_index(drop=True).pipe(lambda x: x[contains_one_label(x, label_col)])\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf2e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T04:33:20.487638Z",
     "start_time": "2021-09-09T04:33:20.460874Z"
    }
   },
   "outputs": [],
   "source": [
    "# format annotations, create experiment with usual structure\n",
    "df, label_cols = one_hot_encode_multi_label_df(quantized_annotations, label_col)\n",
    "train_dev_test_splits = create_multi_label_train_dev_test_splits(\n",
    "    df, text_col=text_col, label_cols=label_cols, split_size=0.4, label_dict=True\n",
    ")  # larger test portions RE: low support\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa102656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T04:33:23.564543Z",
     "start_time": "2021-09-09T04:33:23.552769Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_output_dir = Path(\"/Users/samhardyhey/Desktop/tars_doodle\")\n",
    "create_experiment_output_dir(experiment_output_dir, train_dev_test_splits)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc07b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "python \\\n",
    "-m otso_flair_wrapper.train_multi_label \\\n",
    "-experiment-output-path '/home/rianne/tars_doodle/experiment_a' \\\n",
    "-text-col \"text\" \\\n",
    "-label-col \"label\" \\\n",
    "-model-params '{\"class_task\": \"multi_label\",\"learning_rate\": 0.02,\"mini_batch_size\": 1,\"max_epochs\": 5,\"TARS\":\"True\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b95df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T04:34:56.523163Z",
     "start_time": "2021-09-09T04:34:56.183338Z"
    }
   },
   "outputs": [],
   "source": [
    "res = compare_experiments_barplot(\n",
    "    experiment_paths=[experiment_output_dir],\n",
    "    title=\"TARS eval.\",\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d456613",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "annotation_config = yaml.safe_load(Path(\"../annotation_config.yaml\").read_bytes())\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "toml.load(\"../annotation_config.toml\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947abdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import toml\n",
    "\n",
    "# toml.loads\n",
    "\n",
    "toml.load(Path(\"../annotation_config.toml\").read_text())\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0121aec9e40b71ec9730e04f00957539fc5aa06febb00ef12b9b6cf43c877e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
