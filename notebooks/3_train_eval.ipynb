{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = compare_experiments_barplot(\n",
    "#     experiment_paths=[experiment_output_dir],\n",
    "#     title=\"TARS eval.\",\n",
    "# )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "\n",
    "def label_dictionary_to_label_mat(label_dictionary_list, thresh=0.75):\n",
    "    return (\n",
    "        pd.DataFrame.from_records(list(label_dictionary_list))\n",
    "        .pipe(lambda x: x >= thresh)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "\n",
    "def label_mat_to_label_dictionary(label_mat):\n",
    "    return list(label_mat.to_dict(orient=\"index\").values())\n",
    "\n",
    "\n",
    "def create_multi_label_train_test_splits(\n",
    "    df: pd.core.frame.DataFrame,\n",
    "    label_col: str,\n",
    "    test_size=0.25,\n",
    "):\n",
    "    df[label_col] = df[label_col].apply(\n",
    "        lambda x: eval(x) if type(x) == str else x\n",
    "    )  # string > dict\n",
    "\n",
    "    # threshold, iteratively split\n",
    "    y_df = label_dictionary_to_label_mat(df[label_col])\n",
    "    y_cols = list(y_df.columns)\n",
    "    x_df = df.drop(label_col, axis=1)\n",
    "    x_cols = list(x_df.columns)\n",
    "\n",
    "    x_train, y_train, x_test, y_test = iterative_train_test_split(\n",
    "        x_df.values, y_df.astype(int).values, test_size=test_size\n",
    "    )\n",
    "\n",
    "    # convert back to label object form\n",
    "    y_train = label_mat_to_label_dictionary(pd.DataFrame(y_train, columns=y_cols))\n",
    "    y_test = label_mat_to_label_dictionary(pd.DataFrame(y_test, columns=y_cols))\n",
    "\n",
    "    # re-stack x/y\n",
    "    train = pd.DataFrame(np.column_stack((x_train, y_train))).set_axis(\n",
    "        labels=x_cols + [label_col], axis=\"columns\", inplace=False\n",
    "    )\n",
    "\n",
    "    test = pd.DataFrame(np.column_stack((x_test, y_test))).set_axis(\n",
    "        labels=x_cols + [label_col], axis=\"columns\", inplace=False\n",
    "    )\n",
    "    return train, test\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "CONFIG = yaml.safe_load(\n",
    "    Path(\n",
    "        \"/Users/samhardyhey/Desktop/blog/blog-multi-label/training_config.yaml\"\n",
    "    ).read_bytes()\n",
    ")\n",
    "\n",
    "# 1.1 create splits\n",
    "df = pd.read_csv(CONFIG[\"dataset\"])\n",
    "train, test = create_multi_label_train_test_splits(\n",
    "    df, label_col=CONFIG[\"label_col\"], test_size=CONFIG[\"test_size\"]\n",
    ")\n",
    "test, dev = create_multi_label_train_test_splits(\n",
    "    test, label_col=CONFIG[\"label_col\"], test_size=CONFIG[\"test_size\"]\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 log splits\n",
    "with wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=\"dataset\",\n",
    "    group=CONFIG[\"wandb_group\"],\n",
    "    entity=\"cool_stonebreaker\",\n",
    ") as run:\n",
    "    dataset_artifact = wandb.Artifact(\n",
    "        \"reddit-aus-finance\", type=\"dataset\", description=\"Train, dev, test splits\"\n",
    "    )\n",
    "    dataset_artifact.add(wandb.Table(dataframe=train), name=\"train\")\n",
    "    dataset_artifact.add(wandb.Table(dataframe=dev), name=\"dev\")\n",
    "    dataset_artifact.add(wandb.Table(dataframe=test), name=\"test\")\n",
    "    run.log_artifact(dataset_artifact)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clear_bow.classifier import DictionaryClassifier\n",
    "\n",
    "model_config = CONFIG[\"models\"][0]\n",
    "\n",
    "dc = DictionaryClassifier(\n",
    "    classifier_type=model_config[\"classifier_type\"],\n",
    "    label_dictionary=model_config[\"label_dictionary\"],\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train/test performance > log as model artefacts?\n",
    "dev_pred = dev.assign(pred=lambda x: x[CONFIG[\"text_col\"]].apply(dc.predict_single))\n",
    "test_pred = test.assign(pred=lambda x: x[CONFIG[\"text_col\"]].apply(dc.predict_single))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = label_dictionary_to_label_mat(test_pred).columns.tolist()\n",
    "class_report = classification_report(\n",
    "    label_dictionary_to_label_mat(test[CONFIG[\"label_col\"]]),\n",
    "    label_dictionary_to_label_mat(test_pred),\n",
    "    target_names=label_names,\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "# reduce output into wandb?\n",
    "slim_class_report = (\n",
    "    pd.DataFrame(class_report)\n",
    "    .T.reset_index()\n",
    "    .pipe(lambda x: x[x[\"index\"].isin(label_names)])\n",
    "    .pipe(lambda x: x[[\"index\", \"f1-score\", \"support\"]])\n",
    "    .rename(mapper={\"index\": \"label\"}, axis=\"columns\", inplace=False)\n",
    "    .set_index(\"label\")\n",
    "    .to_dict(orient=\"index\")\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock seperate model runs, within the same group\n",
    "with wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=\"dictionary_classifier\",\n",
    "    group=CONFIG[\"wandb_group\"],\n",
    "    entity=\"cool_stonebreaker\",\n",
    ") as run:\n",
    "    wandb.config.model = model_config[\"model\"]\n",
    "\n",
    "    dev_preds_artifact = wandb.Artifact(\n",
    "        \"dev_preds\", type=\"dataset\", description=\"Dev predictions\"\n",
    "    )\n",
    "    dataset_artifact.add(wandb.Table(dataframe=dev_pred), name=\"dev_preds\")\n",
    "\n",
    "    wandb.log(slim_class_report)\n",
    "    wandb.summary[\"test_f1\"] = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "    wandb.summary[\"test_support\"] = class_report[\"weighted avg\"][\"support\"]\n",
    "\n",
    "# seperate models as seperate runs\n",
    "with wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=\"flair_tars\",\n",
    "    group=CONFIG[\"wandb_group\"],\n",
    "    entity=\"cool_stonebreaker\",\n",
    ") as run:\n",
    "    wandb.config.model = \"flair_tars\"\n",
    "\n",
    "    dev_preds_artifact = wandb.Artifact(\n",
    "        \"dev_preds\", type=\"dataset\", description=\"Dev predictions\"\n",
    "    )\n",
    "    dataset_artifact.add(wandb.Table(dataframe=dev_pred), name=\"dev_preds\")\n",
    "\n",
    "    wandb.log(slim_class_report)\n",
    "    wandb.summary[\"test_f1\"] = class_report[\"weighted avg\"][\"f1-score\"]\n",
    "    wandb.summary[\"test_support\"] = class_report[\"weighted avg\"][\"support\"]\n",
    "    wandb.finish()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out for dev purposes\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "for run in api.runs(path=\"cool_stonebreaker/tyre_kick\"):\n",
    "    run = api.run(f\"cool_stonebreaker/tyre_kick/{run.id}\")\n",
    "    run.delete()\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0121aec9e40b71ec9730e04f00957539fc5aa06febb00ef12b9b6cf43c877e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
