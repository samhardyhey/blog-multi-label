{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = compare_experiments_barplot(\n",
    "#     experiment_paths=[experiment_output_dir],\n",
    "#     title=\"TARS eval.\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "\n",
    "def label_dictionary_to_label_mat(label_dictionary_list, thresh=0.75):\n",
    "    return (\n",
    "        pd.DataFrame.from_records(list(label_dictionary_list))\n",
    "        .pipe(lambda x: x >= thresh)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "\n",
    "def label_mat_to_label_dictionary(label_mat):\n",
    "    return list(label_mat.to_dict(orient=\"index\").values())\n",
    "\n",
    "\n",
    "def create_multi_label_train_test_splits(\n",
    "    df: pd.core.frame.DataFrame,\n",
    "    label_col: str,\n",
    "    test_size=0.25,\n",
    "):\n",
    "    df[label_col] = df[label_col].apply(\n",
    "        lambda x: eval(x) if type(x) == str else x\n",
    "    )  # string > dict\n",
    "\n",
    "    # threshold, iteratively split\n",
    "    y_df = label_dictionary_to_label_mat(df[label_col])\n",
    "    y_cols = list(y_df.columns)\n",
    "    x_df = df.drop(label_col, axis=1)\n",
    "    x_cols = list(x_df.columns)\n",
    "\n",
    "    x_train, y_train, x_test, y_test = iterative_train_test_split(\n",
    "        x_df.values, y_df.astype(int).values, test_size=test_size\n",
    "    )\n",
    "\n",
    "    # convert back to label object form\n",
    "    y_train = label_mat_to_label_dictionary(\n",
    "        pd.DataFrame(y_train, columns=y_cols))\n",
    "    y_test = label_mat_to_label_dictionary(\n",
    "        pd.DataFrame(y_test, columns=y_cols))\n",
    "\n",
    "    # re-stack x/y\n",
    "    train = pd.DataFrame(np.column_stack((x_train, y_train))).set_axis(\n",
    "        labels=x_cols + [label_col], axis=\"columns\", inplace=False\n",
    "    )\n",
    "\n",
    "    test = pd.DataFrame(np.column_stack((x_test, y_test))).set_axis(\n",
    "        labels=x_cols + [label_col], axis=\"columns\", inplace=False\n",
    "    )\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def log_dataframe(run, df, name, description):\n",
    "    # any type of df within a run\n",
    "    df_artifact = wandb.Artifact(\n",
    "        name, type=\"dataset\", description=description\n",
    "    )\n",
    "    df_artifact.add(wandb.Table(dataframe=df), name=name)\n",
    "    run.log_artifact(df_artifact)\n",
    "\n",
    "\n",
    "def create_classification_report(test, test_pred):\n",
    "    label_names = label_dictionary_to_label_mat(\n",
    "        test_pred.pred).columns.tolist()\n",
    "    class_report_dict = classification_report(label_dictionary_to_label_mat(\n",
    "        test[CONFIG[\"label_col\"]]), label_dictionary_to_label_mat(test_pred.pred), target_names=label_names, output_dict=True,)\n",
    "    return (pd.DataFrame(class_report_dict)\n",
    "            .T\n",
    "            .reset_index()\n",
    "            .rename(mapper={\"index\": \"label\"}, axis=\"columns\", inplace=False))\n",
    "\n",
    "\n",
    "def create_slim_class_report(class_report):\n",
    "    return (class_report\n",
    "            .query('label in @label_names')\n",
    "            .pipe(lambda x: x[[\"label\", \"f1-score\", \"support\"]])\n",
    "            .set_index(\"label\")\n",
    "            .to_dict(orient=\"index\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "CONFIG = yaml.safe_load(\n",
    "    Path(\n",
    "        \"/Users/samhardyhey/Desktop/blog/blog-multi-label/training_config.yaml\"\n",
    "    ).read_bytes()\n",
    ")\n",
    "\n",
    "# 1.1 create splits\n",
    "df = pd.read_csv(CONFIG[\"dataset\"])\n",
    "train, test = create_multi_label_train_test_splits(\n",
    "    df, label_col=CONFIG[\"label_col\"], test_size=CONFIG[\"test_size\"]\n",
    ")\n",
    "test, dev = create_multi_label_train_test_splits(\n",
    "    test, label_col=CONFIG[\"label_col\"], test_size=CONFIG[\"test_size\"]\n",
    ")\n",
    "\n",
    "# # 1.2 log splits\n",
    "# with wandb.init(\n",
    "#     project=CONFIG[\"wandb_project\"],\n",
    "#     name=\"reddit_aus_finance\",\n",
    "#     group=CONFIG[\"wandb_group\"],\n",
    "#     entity=\"cool_stonebreaker\",\n",
    "# ) as run:\n",
    "#     log_dataframe(run, train, \"train_split\", \"Train split\")\n",
    "#     log_dataframe(run, dev, \"dev_split\", \"Dev split\")\n",
    "#     log_dataframe(run, test, \"test_split\", \"Test split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dictionary_classifier'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sklearn_svm'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'flair_tars'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in CONFIG['models']:\n",
    "    model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from clear_bow.classifier import DictionaryClassifier\n",
    "\n",
    "\n",
    "def fit_and_log_dictionary_classifier(train, dev, test, model_config):\n",
    "    with wandb.init(\n",
    "        project=CONFIG[\"wandb_project\"],\n",
    "        name=model_config['model'],\n",
    "        group=CONFIG[\"wandb_group\"],\n",
    "        entity=CONFIG[\"wandb_entity\"],\n",
    "    ) as run:\n",
    "        wandb.config.model = model_config['model']\n",
    "        wandb.config.group = CONFIG[\"wandb_group\"]\n",
    "\n",
    "        # instantiate\n",
    "        dc = DictionaryClassifier(\n",
    "            classifier_type=model_config[\"classifier_type\"],\n",
    "            label_dictionary=model_config[\"label_dictionary\"],\n",
    "        )\n",
    "\n",
    "        # predict/evaluate\n",
    "        test_preds = test.assign(\n",
    "            pred=lambda x: x[CONFIG[\"text_col\"]].apply(dc.predict_single))\n",
    "        class_report = create_classification_report(test, test_preds)\n",
    "        class_report_slim = create_classification_report(class_report)\n",
    "\n",
    "        # log\n",
    "        log_dataframe(run, test_preds, \"test_preds\", \"Test predictions\")\n",
    "        run.log(class_report_slim)\n",
    "        run.summary[\"test_f1\"] = class_report.query(\n",
    "            'label == \"weighted avg\"')['f1-score'].iloc[0]\n",
    "        run.summary[\"test_support\"] = class_report.query(\n",
    "            'label == \"weighted avg\"')['support'].iloc[0]\n",
    "\n",
    "\n",
    "def fit_and_log_linear_svc(train, dev, test, model_config):\n",
    "    with wandb.init(\n",
    "        project=CONFIG[\"wandb_project\"],\n",
    "        name=model_config['model'],\n",
    "        group=CONFIG[\"wandb_group\"],\n",
    "        entity=CONFIG[\"wandb_entity\"],\n",
    "    ) as run:\n",
    "        wandb.config.model = model_config['model']\n",
    "        wandb.config.group = CONFIG[\"wandb_group\"]\n",
    "\n",
    "        # define a basic pipeline\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"tfidf\", TfidfVectorizer()),\n",
    "                (\"vt\", VarianceThreshold()),\n",
    "                (\"linear_svc\", BinaryRelevance(LinearSVC())),\n",
    "            ]\n",
    "        )\n",
    "        pipeline_param_grid = {\n",
    "            \"C\": [0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "            \"class_weight\": [\"balanced\", None],\n",
    "            \"max_iter\": [100, 250, 500, 750, 1000],\n",
    "            \"random_state\": [42],\n",
    "        }\n",
    "\n",
    "        # fit\n",
    "        train_dev = pd.concat([train, dev], sort=True)\n",
    "        pipeline.fit(train_dev[CONFIG['text_col']], label_dictionary_to_label_mat(\n",
    "            train_dev[CONFIG['label_col']]))\n",
    "\n",
    "        # predict/evaluate\n",
    "        test_preds = [dict(zip(test.columns.values, pipeline.predict(\n",
    "            [e])[0].toarray()[0])) for e in test[CONFIG['text_col']].tolist()]\n",
    "\n",
    "        class_report = create_classification_report(test, test_preds)\n",
    "        class_report_slim = create_classification_report(class_report)\n",
    "\n",
    "        # log\n",
    "        log_dataframe(run, test_preds, \"test_preds\", \"Test predictions\")\n",
    "        run.log(class_report_slim)\n",
    "        run.summary[\"test_f1\"] = class_report.query(\n",
    "            'label == \"weighted avg\"')['f1-score'].iloc[0]\n",
    "        run.summary[\"test_support\"] = class_report.query(\n",
    "            'label == \"weighted avg\"')['support'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'dictionary_classifier',\n",
       " 'classifier_type': 'multi_label',\n",
       " 'label_dictionary': {'regulation': ['asic', 'government', 'federal', 'tax'],\n",
       "  'contribution': ['contribution',\n",
       "   'concession',\n",
       "   'personal',\n",
       "   'after tax',\n",
       "   '10%',\n",
       "   '10.5%'],\n",
       "  'covid': ['covid', 'lockdown', 'downturn', 'effect'],\n",
       "  'retirement': ['retire', 'house', 'annuity', 'age'],\n",
       "  'fund': ['unisuper',\n",
       "   'aus super',\n",
       "   'australian super',\n",
       "   'sun super',\n",
       "   'qsuper',\n",
       "   'rest',\n",
       "   'cbus']}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'sklearn_svm'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'flair_tars'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in CONFIG['models']:\n",
    "    if model['name'] == 'dictionary_classifier':\n",
    "        fit_and_log_dictionary_classifier(train, dev, test, model)\n",
    "\n",
    "    elif model['name'] == 'sklearn_linear_svc':\n",
    "        fit_and_log_linear_svc(train, dev, test, model)\n",
    "\n",
    "    else:\n",
    "        print(f\"Unsupported model: {model['name']} found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0bb0660ada4b8b8a81613069fbdecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.03335033257802327, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/samhardyhey/Desktop/blog/blog-multi-label/notebooks/wandb/run-20220921_084222-1x27fi4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cool_stonebreaker/tyre_kick/runs/1x27fi4i\" target=\"_blank\">flair_tars</a></strong> to <a href=\"https://wandb.ai/cool_stonebreaker/tyre_kick\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce3bc49d9304f65947c1f57c79fc905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_f1</td><td>0.21818</td></tr><tr><td>test_support</td><td>11.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">flair_tars</strong>: <a href=\"https://wandb.ai/cool_stonebreaker/tyre_kick/runs/1x27fi4i\" target=\"_blank\">https://wandb.ai/cool_stonebreaker/tyre_kick/runs/1x27fi4i</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220921_084222-1x27fi4i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seperate models as seperate runs\n",
    "with wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=\"flair_tars\",\n",
    "    group=CONFIG[\"wandb_group\"],\n",
    "    entity=\"cool_stonebreaker\",\n",
    ") as run:\n",
    "    wandb.config.model = model_config['model']\n",
    "\n",
    "    # log dev/pred preds\n",
    "    log_dataframe(run, dev_pred, \"dev_preds\", \"Dev predictions\")\n",
    "    log_dataframe(run, test_pred, \"test_preds\", \"Test predictions\")\n",
    "\n",
    "    run.log(slim_class_report)\n",
    "    run.summary[\"test_f1\"] = class_report.query('label == \"weighted avg\"')['f1-score'].iloc[0]\n",
    "    run.summary[\"test_support\"] = class_report.query('label == \"weighted avg\"')['support'].iloc[0]\n",
    "\n",
    "\n",
    "# # seperate models as seperate runs\n",
    "# with wandb.init(\n",
    "#     project=CONFIG[\"wandb_project\"],\n",
    "#     name=\"flair_tars\",\n",
    "#     group=CONFIG[\"wandb_group\"],\n",
    "#     entity=\"cool_stonebreaker\",\n",
    "# ) as run:\n",
    "#     wandb.config.model = \"flair_tars\"\n",
    "\n",
    "#     # log dev/pred preds\n",
    "#     log_dataframe(run, dev_pred, \"dev_preds\", \"Dev predictions\")\n",
    "#     log_dataframe(run, test_pred, \"test_preds\", \"Test predictions\")\n",
    "\n",
    "#     run.log(slim_class_report)\n",
    "#     run.summary[\"test_f1\"] = class_report.query('label == \"weighted avg\"')['f1-score'].iloc[0]\n",
    "#     run.summary[\"test_support\"] = class_report.query('label == \"weighted avg\"')['support'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out for dev purposes\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "for run in api.runs(path=\"cool_stonebreaker/tyre_kick\"):\n",
    "    run = api.run(f\"cool_stonebreaker/tyre_kick/{run.id}\")\n",
    "    run.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.10.0 tenacity-8.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd487bf160>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'some interesting numbers')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamhardyhey\u001b[0m (\u001b[33mcool_stonebreaker\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/samhardyhey/Desktop/blog/blog-multi-label/notebooks/wandb/run-20220921_091916-62zaje33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cool_stonebreaker/blog-multi-label-notebooks/runs/62zaje33\" target=\"_blank\">summer-pine-3</a></strong> to <a href=\"https://wandb.ai/cool_stonebreaker/blog-multi-label-notebooks\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baba6887dba42c186dd79b6583479f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">summer-pine-3</strong>: <a href=\"https://wandb.ai/cool_stonebreaker/blog-multi-label-notebooks/runs/62zaje33\" target=\"_blank\">https://wandb.ai/cool_stonebreaker/blog-multi-label-notebooks/runs/62zaje33</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220921_091916-62zaje33/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "fibonacci = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
    "plt.plot(fibonacci)\n",
    "plt.ylabel('some interesting numbers')\n",
    "\n",
    "# Initialize run\n",
    "with wandb.init(\n",
    "        project=CONFIG[\"wandb_project\"],\n",
    "        name=\"flair_tars\",\n",
    "        group=CONFIG[\"wandb_group\"],\n",
    "        entity=\"cool_stonebreaker\",\n",
    "    ) as run:\n",
    "\n",
    "    # Log plot object\n",
    "    wandb.log({\"plot\": plt})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb0121aec9e40b71ec9730e04f00957539fc5aa06febb00ef12b9b6cf43c877e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
